\documentclass[a4paper,11pt]{article}

% --- PREAMBLE ---
% Géométrie et Langue
\usepackage[a4paper, hmargin=2.5cm, vmargin=2.5cm]{geometry}

% Configuration des polices pour pdfLaTeX (Compatibilité standard)
\usepackage[utf8]{inputenc} % Encodage du fichier source
\usepackage[T1]{fontenc}    % Encodage des polices de sortie
\usepackage{lmodern}        % Police vectorielle de haute qualité (remplace Computer Modern)
\usepackage[french]{babel}  % Gestion de la langue française

% Packages graphiques et mathématiques
\usepackage{graphicx}
\usepackage{array}
\usepackage[table]{xcolor}
\usepackage{amsmath, amssymb}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{tikz}
\usepackage{float}
\usepackage{changepage}
\usepackage{adjustbox}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}

% Configuration des liens
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=blue,
    pdftitle={Rapport MoE},
    pdfpagemode=FullScreen,
}

% Configuration du code
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonStyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    rulecolor=\color{gray!50}
}
\lstset{style=pythonStyle}

\begin{document}

% --- PAGE DE GARDE (Style Université de Bordeaux) ---
\begin{titlepage}
    \newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 
    
    \center 
    
    \textsc{\LARGE Université de Bordeaux}\\[1.5cm] 
    \textsc{\Large Master 2 Informatique - Intelligence Artificielle}\\[0.5cm] 
    \textsc{\large Architectures de Réseaux de neurones avancées}\\[0.5cm]
    
    \vspace{1cm}
    \HRule \\[0.4cm]
    { \huge \bfseries Implémentation et Analyse d'un Mixture of Experts (MoE) sur CIFAR-10}\\[0.4cm] 
    \HRule \\[1.5cm]
    
    \begin{minipage}{0.4\textwidth}
        \begin{flushleft} \large
        \emph{Auteurs :}\\
        \textbf{Arthur Macdonald\\ Niels Roudeau}\\
        \end{flushleft}
    \end{minipage}
    ~
    \begin{minipage}{0.4\textwidth}
        \begin{flushright} \large
            Année Universitaire 2025-2026
        \end{flushright}
    \end{minipage}\\[2cm]
    
    \vfill 
    
\end{titlepage}

\tableofcontents
\newpage

% --- CONTENU DU RAPPORT ---

\section{Présentation du projet}

L'objectif de ce projet est d'implémenter, d'entraîner et d'analyser une architecture neuronale de type \textbf{Mixture of Experts (MoE)} appliquée au jeu de données CIFAR-10. 

Les modèles modernes d'apprentissage profond cherchent constamment à augmenter leur capacité (nombre de paramètres) pour capturer des nuances plus fines dans les données. Cependant, cette augmentation de taille s'accompagne traditionnellement d'une explosion des coûts de calcul. L'approche MoE, popularisée par Shazeer et al. (2017) et récemment réactualisée par Mixtral, propose une solution via le \textbf{Calcul Conditionnel} (\textit{Conditional Computation}).

Dans ce rapport, nous détaillerons :
\begin{itemize}
    \item Nos choix d'architecture (Gating Network, Experts Convolutionnels).
    \item L'implémentation modulaire en PyTorch.
    \item Les résultats expérimentaux comparant notre MoE à une \textit{Baseline} Dense, en analysant spécifiquement la spécialisation des experts.
\end{itemize}

\section{Architecture du Projet}

Le code complet et la structure du projet sont organisés de manière modulaire pour séparer la logique de modélisation, l'entraînement et l'analyse.

Les composants principaux sont :
\begin{itemize}
    \item \textbf{Le noyau du modèle} dans le dossier \texttt{models/} :
        \begin{itemize}
            \item \texttt{gating.py} : Définit le réseau de routage (Router).
            \item \texttt{experts.py} : Définit l'architecture des sous-réseaux experts.
            \item \texttt{moe\_model.py} : Orchestre le tout (Manager) et gère le routage Top-k.
            \item \texttt{dense\_baseline.py} : Une architecture CNN classique servant de point de comparaison.
        \end{itemize}
    \item \textbf{Les scripts d'exécution} :
        \begin{itemize}
            \item \texttt{train.py} : Script CLI pour lancer les entraînements avec gestion des hyperparamètres.
            \item \texttt{experiments.ipynb} : Notebook Jupyter contenant le protocole expérimental complet, les boucles d'entraînement et la génération des visualisations.
        \end{itemize}
    \item \textbf{Utilitaires} :
        \begin{itemize}
            \item \texttt{data\_loader.py} : Gestion du dataset CIFAR-10 et augmentations.
            \item \texttt{visualization.py} : Génération des courbes de perte et des heatmaps d'experts.
        \end{itemize}
\end{itemize}

\section{Définition du Modèle MoE}

\subsection{Représentation et Routage}
Contrairement à un modèle dense où chaque couche traite l'intégralité de l'entrée, notre modèle MoE sélectionne dynamiquement quels paramètres utiliser.

L'entrée est une image $x$ de dimension $(3, 32, 32)$.
\begin{enumerate}
    \item \textbf{Le Gating Network ($g(x)$)} : Un petit CNN analyse l'image et produit un vecteur de probabilité sur $N$ experts. Nous utilisons un \textit{Noisy Top-k Gating} (bruit gaussien ajouté à l'entraînement, déterministe à l'inférence).
    \item \textbf{Sélection (Sparse)} : Seuls les indices $k$ ayant les plus hautes probabilités sont retenus.
    \item \textbf{Calcul Expert} : L'image est passée uniquement à travers les experts sélectionnés $E_i(x)$.
    \item \textbf{Agrégation} : La sortie est la somme pondérée des sorties des experts :
    $$ y = \sum_{i \in \text{Top-k}} g(x)_i \cdot E_i(x) $$
\end{enumerate}

\subsection{Architecture des Experts}
Chaque expert est un CNN indépendant conçu pour être plus spécialisé qu'un réseau généraliste.
\begin{itemize}
    \item \textbf{Structure :} 3 blocs de convolution (32 $\rightarrow$ 64 $\rightarrow$ 128 filtres) avec MaxPool, suivis d'une tête linéaire.
    \item \textbf{Intuition :} Certains experts peuvent se spécialiser sur les textures (animaux), d'autres sur les formes rigides (véhicules).
\end{itemize}

\subsection{Load Balancing (Équilibrage de charge)}
Un défi majeur des MoE est d'éviter un scénario de déséquilibre de la spécialisation des experts (Expert Specialization Collapse), où le routeur envoie toutes les images vers le même expert, perdant ainsi son aspect de spécialisation.\newline
Pour contrer cela, nous avons implémenté une \textbf{perte auxiliaire} ($\mathcal{L}_{aux}$) ajoutée à la fonction de coût principale :
$$ \mathcal{L}_{total} = \mathcal{L}_{CrossEntropy} + \lambda \cdot \mathcal{L}_{aux} $$
Cette perte pénalise une distribution inégale de l'utilisation des experts au sein d'un batch (basée sur l'Erreur Quadratique Moyenne - MSE - par rapport à une distribution uniforme).

\section{Résultats expérimentaux}

L'ensemble des expériences et résultats détaillés sont présentés dans le notebook \textbf{experiments.ipynb}. Nous résumons ici les protocoles choisis et conclusions essentielles.

\subsection{Protocole}
\begin{itemize}
    \item \textbf{Données :} CIFAR-10 (45k train / 5k val / 10k test).
    \item \textbf{\textit{Baseline} :} Un CNN dense de largeur ajustable (\texttt{width\_multiplier}) pour correspondre soit au nombre de paramètres total du MoE, soit à son coût d'inférence (FLOPs).
    \item \textbf{MoE Configuration :} Différents modèles de MoE avec des hyperparamètres variables.
\end{itemize}

\subsection{Comparaison de Performance}

\begin{figure}[H]
    \centering
    \setlength{\fboxsep}{10pt}
    \setlength{\fboxrule}{1pt}
    \includegraphics[width=0.9\textwidth]{graph.png}
    \caption{Evolution de la précision en fonction du nombre de paramètres des modèles}
    \label{fig:graph.png}
\end{figure}


\begin{itemize}
    \item \textbf{Evolution de la \textit{baseline} :}\\
    Le modèle \textit{baseline} semble suivre une courbe logarithmique: augmenter le nombre de paramètre rend le modèle plus robuste mais est de plus en plus gourmand.
    \item \textbf{Variations de l'accuracy des MoE :}\\
   La performance d'un MoE ne dépend pas uniquement de sa taille, mais surtout de l'équilibre entre les experts(aux) et le routage. 
   Sans contrainte (Aux 0.0), le MoE se limite à un seul expert, c'est ce qu'on veut éviter avec la perte auxiliaire.\\ 
   L'introduction d'une perte auxiliaire permet de forcer l'usage de plus d'experts, mais cela ne devient réellement bénéfique que si l'on active plusieurs experts simultanément (Top-k à 2 ou 3).\\
   Cela crée un effet d'ensemble efficace, permettant par exemple au modèle Top-3 de rivaliser avec un modèle dense deux fois plus gros tout en étant moins coûteux.\\
   Enfin, augmenter aveuglément le nombre total d'experts n'est pas automatiquement bénéfique : si la tâche n'est pas assez complexe, diviser le modèle en trop de sous-parties devient inutile.
\end{itemize}

\subsection{Analyse de la Spécialisation}

\begin{figure}[H]
    \centering
    \setlength{\fboxsep}{10pt}
    \setlength{\fboxrule}{1pt}
    \includegraphics[width=0.9\textwidth]{matrice.png}
    \caption{Heatmap de routage : Fréquence d'activation de chaque expert pour chaque classe.}
    \label{fig:matrice.png}
\end{figure}

\begin{figure}[H]
    \centering
    \setlength{\fboxsep}{10pt}
    \setlength{\fboxrule}{1pt}
    \includegraphics[width=0.9\textwidth]{stability.png}
    \caption{Utilisation d'experts}
    \label{fig:stability.png}
\end{figure}

\textbf{Analyse :}

L'histogramme d'utilisation (Fig. \ref{fig:stability.png}) montre une répartition relativement équilibrée de la charge, bien que non uniforme. Tous les experts sont sollicités, avec une charge variant de 17,5\% (Expert 2) à 30,0\% (Expert 1). On distingue deux groupes : les experts à forte charge (E1 et E3, $\approx 30\%$ chacun) et ceux à charge modérée (E0 et E2).

Cependant, c'est la matrice de spécialisation (Fig. \ref{fig:matrice.png}) qui révèle la dynamique réelle du modèle. Contrairement à une simple polyvalence, on observe une \textbf{segmentation sémantique} claire entre les classes "Véhicules" et "Animaux" du dataset CIFAR-10.

\begin{itemize}
    \item \textbf{Groupe "Animaux" (Experts 0 et 3) :}
    \begin{itemize}
        \item \textbf{L'Expert 0} est un "spécialiste pointu". Il présente des pics d'activation très intenses (couleur jaune) pour la classe 6 (Grenouille) et la classe 4 (Cerf), et une activité modérée pour les autres animaux.
        \item \textbf{L'Expert 3} agit comme un "généraliste de catégorie". Bien qu'il n'ait pas de pic jaune vif, il maintient une activation constante (bleu/vert) sur l'ensemble des classes animales (2 à 7), tout en étant très peu actif sur les véhicules.
    \end{itemize}
    
    \item \textbf{Groupe "Véhicules" (Experts 1 et 2) :}
    \begin{itemize}
        \item \textbf{L'Expert 2}, bien que le moins utilisé globalement (17,5\%), est hautement spécialisé. Il gère prioritairement la classe 0 (Avion) et la classe 8 (Bateau).
        \item \textbf{L'Expert 1} est le pendant de l'Expert 3 pour les véhicules. Il montre une activation diffuse mais présente sur les classes mécaniques (1, 8, 9), expliquant sa forte utilisation globale (30\%) sans pour autant avoir de "pic" unique.
    \end{itemize}
\end{itemize}

\section{Discussion Critique}

Bien que les résultats obtenus confirment le potentiel des architectures \textit{Mixture of Experts} sur CIFAR-10, la mise en œuvre de ce genre de modèle nous a permis de mettre le doigt sur quelques limitations.

\subsection{Le défi de l'effondrement des experts (Expert Collapse)}
La difficulté majeure que nous avons rencontrée lors de l'entraînement est le phénomène d'effondrement des experts (\textit{Expert Collapse}). Sans mécanisme de régulation, le réseau de routage tend naturellement vers une solution triviale où il attribue systématiquement la probabilité maximale aux mêmes experts (souvent un ou deux), laissant les autres inactifs.
\begin{itemize}
    \item \textbf{Conséquence :} Le modèle redevient un réseau dense standard mais avec beaucoup de paramètres "morts", perdant tout l'intérêt de la capacité conditionnelle.
    \item \textbf{Contre-mesure :} L'introduction de la perte auxiliaire (ou d'autres stratégies similaire) est indispensable pour forcer une distribution uniforme. Cependant, nous avons observé que cette méthode introduit un conflit d'objectifs : si son poids ($\lambda$) est trop élevé, le routeur privilégie l'équilibre au détriment de la précision de classification (les experts reçoivent des données qui ne leur correspondent pas).
\end{itemize}

\subsection{Sensibilité aux Hyperparamètres}
Contrairement aux CNN classiques où augmenter la taille améliore souvent la performance de manière monotone, les MoE sont extrêmement sensibles au réglage des hyperparamètres structurels :
\begin{itemize}
    \item \textbf{Le Top-k :} Un $k=1$ (Hard Routing) est très rapide mais instable car le gradient ne se propage que vers un seul expert. Un $k=2$ ou $k=3$ permet un effet d'ensemble bénéfique et une meilleure propagation de l'erreur, comme observé dans nos résultats, mais augmente le coût de calcul.
    \item \textbf{Nombre d'experts :} Augmenter le nombre d'experts n'est pas linéairement corrélé à la performance. Sur un dataset simple comme CIFAR-10, nous avons constaté qu'un nombre excessif d'experts dilue l'information : chaque expert voit trop peu d'échantillons pour apprendre une spécialisation robuste (comme la distinction animaux/véhicules).
\end{itemize}

\subsection{Coût d'Entraînement et Convergence}
Une distinction importante doit être faite entre l'efficacité à l'inférence et le coût de l'apprentissage. Si le MoE est efficace lors de son utilisation (inférence \textit{sparse}), son entraînement est paradoxalement plus lourd qu'un modèle dense équivalent :
\begin{itemize}
    \item \textbf{Convergence lente :} Le modèle possédant un très grand nombre de paramètres totaux, et chaque expert ne voyant qu'une fraction des données (routage conditionnel), la convergence est plus lente. Il faut plus d'époques pour que chaque sous-réseau apprenne efficacement sa spécialisation.
    \item \textbf{Impact énergétique :} Cette lenteur à l'entraînement pose un problème écologique et économique. L'entraînement étant l'étape la plus énergivore du cycle de vie d'un modèle (bien plus coûteuse que l'inférence), le surcoût énergétique nécessaire pour entraîner l'immense quantité de paramètres du MoE peut réduire, voire annuler, les gains d'efficacité obtenus lors du déploiement, à moins que le modèle ne soit utilisé à très grande échelle par la suite.
\end{itemize}

\subsection{Limites Matérielles et Perspectives}
Bien que le MoE réduise les FLOPs théoriques (coût d'inférence), il n'allège pas la charge mémoire (VRAM). Tous les paramètres de tous les experts doivent être chargés en mémoire, même si seulement une fraction est utilisée par itération.
Une piste d'amélioration pour des travaux futurs serait de remplacer nos experts convolutifs simples par des blocs Transformer complets (type \textit{Switch Transformer}), où le gain de capacité des MoE est plus documenté, notamment pour la modélisation de séquences complexes.

\section{Bibliographie}

\begin{thebibliography}{9}

\bibitem{wang2024}
Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, Damai Dai.
\textit{Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts}.
arXiv preprint arXiv:2408.15664, 2024.
\url{https://arxiv.org/abs/2408.15664}

\bibitem{datacamp}
DataCamp.
\textit{What is Mixture of Experts (MoE)?}
Blog technique.
\url{https://www.datacamp.com/blog/mixture-of-experts-moe}

\bibitem{hughes2025}
Chris Hughes.
\textit{How MoE Models Actually Learn: A Guide to Auxiliary Losses and Expert Balancing}.
Medium, 2025.
\url{https://medium.com/@chris.p.hughes10/how-moe-models-actually-learn-a-guide-to-auxiliary-losses-and-expert-balancing-293084e3f600}

\bibitem{youtube_moe}
Ressource Vidéo.
\textit{Mixture of Experts Explained}.
YouTube.
\url{https://www.youtube.com/watch?v=_P2T4Oi0VxE}

\end{thebibliography}

\end{document}