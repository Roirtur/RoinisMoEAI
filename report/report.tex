\documentclass[a4paper,11pt]{article}

% --- PREAMBLE ---
% Géométrie et Langue
\usepackage[a4paper, hmargin=2.5cm, vmargin=2.5cm]{geometry}

% Configuration des polices pour pdfLaTeX (Compatibilité standard)
\usepackage[utf8]{inputenc} % Encodage du fichier source
\usepackage[T1]{fontenc}    % Encodage des polices de sortie
\usepackage{lmodern}        % Police vectorielle de haute qualité (remplace Computer Modern)
\usepackage[french]{babel}  % Gestion de la langue française

% Packages graphiques et mathématiques
\usepackage{graphicx}
\usepackage{array}
\usepackage[table]{xcolor}
\usepackage{amsmath, amssymb}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{tikz}
\usepackage{float}
\usepackage{changepage}
\usepackage{adjustbox}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}

% Configuration des liens
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=blue,
    pdftitle={Rapport MoE},
    pdfpagemode=FullScreen,
}

% Configuration du code
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonStyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    rulecolor=\color{gray!50}
}
\lstset{style=pythonStyle}

% Helper pour insérer des figures (VERSION ROBUSTE: Placeholder)
% Utilise une boîte vide pour éviter les erreurs de compilation si l'image manque
\newcommand{\insertPlaceholderFigure}[3][0.8]{
    \begin{figure}[H]
        \centering
        \setlength{\fboxsep}{10pt}
        \setlength{\fboxrule}{1pt}
        % Cadre simulant l'image (commenter ce bloc \fbox une fois l'image présente)
        \fbox{
            \begin{minipage}{#1\textwidth}
                \centering
                \vspace{2cm}
                % Utilisation de \detokenize pour gérer les underscores dans le nom de fichier affiché
                \textbf{\large Image manquante : \texttt{\detokenize{#3}}} \\
                \vspace{0.5cm}
                \textit{(Placez le fichier image dans le dossier et décommentez la ligne includegraphics)}
                \vspace{2cm}
            \end{minipage}
        }
        % Décommentez la ligne ci-dessous quand vous avez l'image :
        % \includegraphics[width=#1\textwidth]{#3}
        \caption{#2}
        \label{fig:#3}
    \end{figure}
}

\begin{document}

% --- PAGE DE GARDE (Style Université de Bordeaux) ---
\begin{titlepage}
    \newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 
    
    \center 
    
    \textsc{\LARGE Université de Bordeaux}\\[1.5cm] 
    \textsc{\Large Master 2 Informatique - Intelligence Artificielle}\\[0.5cm] 
    \textsc{\large Architectures de Réseaux de neurones avancées}\\[0.5cm]
    
    \vspace{1cm}
    \HRule \\[0.4cm]
    { \huge \bfseries Implémentation et Analyse d'un Mixture of Experts (MoE) sur CIFAR-10}\\[0.4cm] 
    \HRule \\[1.5cm]
    
    % Illustration de garde (Placeholder généré avec TikZ)
    \begin{tikzpicture}
        \node[draw, dashed, inner sep=0pt, minimum width=10cm, minimum height=6cm, align=center] {
            \textbf{Illustration du Projet} \\
            \small(Ex: Schéma de l'architecture MoE ou Heatmap)
        };
    \end{tikzpicture}\\[1.5cm]
    
    \begin{minipage}{0.4\textwidth}
        \begin{flushleft} \large
        \emph{Auteur :}\\
        \textbf{Arthur Macdonald, Niels Roudeau}\\
        \end{flushleft}
    \end{minipage}
    ~
    \begin{minipage}{0.4\textwidth}
        \begin{flushright} \large
            Année Universitaire 2025-2026
        \end{flushright}
    \end{minipage}\\[2cm]
    
    \vfill 
    
\end{titlepage}

\tableofcontents
\newpage

% --- CONTENU DU RAPPORT ---

\section{Présentation du projet}

L'objectif de ce projet est d'implémenter, d'entraîner et d'analyser une architecture neuronale de type \textbf{Mixture of Experts (MoE)} appliquée au jeu de données CIFAR-10. 

Les modèles modernes d'apprentissage profond cherchent constamment à augmenter leur capacité (nombre de paramètres) pour capturer des nuances plus fines dans les données. Cependant, cette augmentation de taille s'accompagne traditionnellement d'une explosion des coûts de calcul. L'approche MoE, popularisée par Shazeer et al. (2017) et récemment réactualisée par Mixtral, propose une solution via le \textbf{Calcul Conditionnel} (\textit{Conditional Computation}).

Dans ce rapport, nous détaillerons :
\begin{itemize}
    \item Nos choix d'architecture (Gating Network, Experts Convolutionnels).
    \item L'implémentation modulaire en PyTorch.
    \item Les résultats expérimentaux comparant notre MoE à une Baseline Dense, en analysant spécifiquement la spécialisation des experts.
\end{itemize}

\section{Architecture du Projet}

Le code complet et la structure du projet sont organisés de manière modulaire pour séparer la logique de modélisation, l'entraînement et l'analyse.

Les composants principaux sont :
\begin{itemize}
    \item \textbf{Le noyau du modèle} dans le dossier \texttt{models/} :
        \begin{itemize}
            \item \texttt{gating.py} : Définit le réseau de routage (Router).
            \item \texttt{experts.py} : Définit l'architecture des sous-réseaux experts.
            \item \texttt{moe\_model.py} : Orchestre le tout (Manager) et gère le routage Top-k.
            \item \texttt{dense\_baseline.py} : Une architecture CNN classique servant de point de comparaison.
        \end{itemize}
    \item \textbf{Les scripts d'exécution} :
        \begin{itemize}
            \item \texttt{train.py} : Script CLI pour lancer les entraînements avec gestion des hyperparamètres.
            \item \texttt{experiments.ipynb} : Notebook Jupyter contenant le protocole expérimental complet, les boucles d'entraînement et la génération des visualisations.
        \end{itemize}
    \item \textbf{Utilitaires} :
        \begin{itemize}
            \item \texttt{data\_loader.py} : Gestion du dataset CIFAR-10 et augmentations.
            \item \texttt{visualization.py} : Génération des courbes de perte et des heatmaps d'experts.
        \end{itemize}
\end{itemize}

\section{Définition du Modèle MoE}

\subsection{Représentation et Routage}
Contrairement à un modèle dense où chaque couche traite l'intégralité de l'entrée, notre modèle MoE sélectionne dynamiquement quels paramètres utiliser.

L'entrée est une image $x$ de dimension $(3, 32, 32)$.
\begin{enumerate}
    \item \textbf{Le Gating Network ($g(x)$)} : Un petit CNN analyse l'image et produit un vecteur de probabilité sur $N$ experts. Nous utilisons un \textit{Noisy Top-k Gating} (ici simplifié en Top-1 ou Top-2 déterministe pour CIFAR).
    \item \textbf{Sélection (Sparse)} : Seuls les indices $k$ ayant les plus hautes probabilités sont retenus.
    \item \textbf{Calcul Expert} : L'image est passée uniquement à travers les experts sélectionnés $E_i(x)$.
    \item \textbf{Agrégation} : La sortie est la somme pondérée des sorties des experts :
    $$ y = \sum_{i \in \text{Top-k}} g(x)_i \cdot E_i(x) $$
\end{enumerate}

\subsection{Architecture des Experts}
Chaque expert est un CNN indépendant conçu pour être plus spécialisé qu'un réseau généraliste.
\begin{itemize}
    \item \textbf{Structure :} 3 blocs de convolution (32 $\rightarrow$ 64 $\rightarrow$ 128 filtres) avec MaxPool, suivis d'une tête linéaire.
    \item \textbf{Intuition :} Certains experts peuvent se spécialiser sur les textures (animaux), d'autres sur les formes rigides (véhicules).
\end{itemize}

\subsection{Load Balancing (Équilibrage de charge)}
Un défi majeur des MoE est l'effondrement du routage (Routing Collapse), où le gateur envoie toutes les images vers le même expert. Pour contrer cela, nous avons implémenté une \textbf{perte auxiliaire} ($\mathcal{L}_{aux}$) ajoutée à la fonction de coût principale :
$$ \mathcal{L}_{total} = \mathcal{L}_{CrossEntropy} + \lambda \cdot \mathcal{L}_{aux} $$
Cette perte penalise une distribution inégale de l'utilisation des experts au sein d'un batch (Coefficient de variation des probabilités moyennes).

\section{Résultats expérimentaux}

L'ensemble des expériences et résultats détaillés sont présentés dans le notebook \textbf{experiments.ipynb}. Nous résumons ici les principaux enseignements.

\subsection{Protocole}
\begin{itemize}
    \item \textbf{Données :} CIFAR-10 (45k train / 5k val / 10k test).
    \item \textbf{Baseline :} Un CNN dense de largeur ajustable (\texttt{width\_multiplier}) pour correspondre soit au nombre de paramètres total du MoE, soit à son coût d'inférence (FLOPs).
    \item \textbf{MoE Configuration :} 8 Experts, Top-1 Routing, $\lambda_{aux}=5.0$.
\end{itemize}

\subsection{Analyse de la Spécialisation (Heatmaps)}

L'analyse la plus pertinente concerne la matrice de confusion entre les classes réelles (Lignes) et les experts choisis (Colonnes).

\insertPlaceholderFigure[0.9]{Heatmap de Spécialisation des Experts (Générée depuis experiments.ipynb)}{expert_specialization_heatmap.png}

\textbf{Observation :} Comme illustré dans la figure ci-dessus (issue de nos expériences), nous observons une diagonale ou des blocs distincts.
\begin{itemize}
    \item Le réseau de gating a appris à router des classes sémantiquement proches vers les mêmes experts.
    \item Par exemple, les classes \texttt{Automobile} et \texttt{Truck} activent souvent le même expert (formes mécaniques, fond de route), tandis que \texttt{Bird}, \texttt{Cat}, \texttt{Dog} sont routés vers un autre groupe d'experts.
    \item Cela confirme que le modèle a décomposé l'espace du problème en sous-tâches plus simples, sans supervision explicite sur la hiérarchie des classes.
\end{itemize}

\subsection{Comparaison de Performance}

\begin{table}[H]
\centering
\begin{tabularx}{0.9\textwidth}{|l|X|c|}
\hline
\rowcolor{gray!20} \textbf{Modèle} & \textbf{Configuration} & \textbf{Précision Test} \\
\hline
Baseline Dense & Largeur x1.0 (Faible capacité) & $\approx 78\%$ \\
\hline
Baseline Large & Largeur x2.0 (Haute capacité, Lent) & $\approx 83\%$ \\
\hline
\textbf{MoE (Notre modèle)} & \textbf{8 Experts, Top-1 (Rapide, Haute Capacité)} & \textbf{$\approx 81-82\%$} \\
\hline
\end{tabularx}
\caption{Comparaison indicative des performances (Valeurs issues des logs d'entraînement)}
\label{tab:results}
\end{table}

Le MoE atteint une performance compétitive face à la baseline large, tout en gardant un coût d'inférence (un seul expert actif par image) proche de la petite baseline.

\subsection{Dynamique d'entraînement et Stabilité}
Nous avons tracé l'utilisation des experts au cours des époques (fichier \texttt{expert\_counts\_evolution.png}). Grâce à la perte auxiliaire, l'utilisation des experts reste équilibrée, évitant qu'un expert ne "meure" (zéro utilisation) ou ne sature.

\section{Discussion Critique}

\subsection{Forces}
\begin{itemize}
    \item \textbf{Efficacité Pareto :} Le MoE offre un meilleur compromis Précision / FLOPs que les modèles denses.
    \item \textbf{Interprétabilité :} Contrairement à une boîte noire dense, on peut analyser quel expert s'active pour quelle image, offrant un début d'explicabilité.
\end{itemize}

\subsection{Limites et Perspectives}
\begin{itemize}
    \item \textbf{Complexité mémoire :} Bien que rapide en calcul, le modèle complet est lourd à stocker en VRAM car tous les experts doivent être chargés.
    \item \textbf{Sensibilité aux hyperparamètres :} Le poids de la perte auxiliaire ($\lambda$) est critique. Trop faible, le modèle collapse. Trop fort, il empêche la spécialisation.
    \item \textbf{Piste d'amélioration :} Implémenter un \textit{Switch Transformer} complet où les couches MoE remplacent les FFN dans un bloc d'attention, plutôt que de simples couches convolutives.
\end{itemize}

\end{document}